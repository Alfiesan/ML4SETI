{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true
            },
            "source": "# Welcome to the SETI Institute Code Challenge!\n\nThis first tutorial will explain a little bit on what the data is and where to get it.\n\n# Introduction\n\nFor the Code Challenge, you will be using the **\"primary\" data set**, as we've called it. The primary data set is   \n\n  * labeled data set of 350,000 simulated signals\n  * 7 different labels, or \"signal classifications\"\n  * total of about 128 GB of data\n\nThis data set should be used to train your models. **You do not need to use all the data to train your models if you do not want to or need to consume the entire set**. There are also a `small` and a `medium` sized subset of these primary data files. \n\n\n### Data File Format\n\nEach data file has a simple format: \n\n * File name: &lt;UUID&gt;.dat\n * Content:\n   * JSON header in the first line that contains:\n      * UUID\n      * signal_classification (label)\n   * followed by stream complex-valued time-series data. \n\nThe [`ibmseti` Python package](https://pypi.python.org/pypi/ibmseti/) is available to assist in reading this data and performing some basic operations for you. \n\n\n### Data Index Files\n\nFor all data sets, there exists an **index** file. That file is a comma-separated value (CSV) file. Each row holds the UUID, signal_classification (label) for a simulation file in the data set. You can use these index files in a few different ways (from using to keep track of your downloads, to facilitate parallelization of your analysis on Spark).\n\nExample content:\n\n```\n  UUID,SIGNAL_CLASSIFICATION\n  b1...2e,narrowband\n  d8...e4,squiggle\n```\n\n\n<hr>\n# Getting started (\"Basic\") Data Set\n\nThere is also a second, simple and clean data set that you may use for warmup, which we call the **\"basic\" data set**. This basic set should be used as a sanity check and for very early-stage prototyping. We recommend that everybody starts with this. \n\n * Only 4 different signal classifications\n * 1,000 simulation files for each class: 4,000 files total\n * Data ZIP file (~ 1.1 GB) \n   * File 1/1: https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_basic_v2/basic4.zip\n * Index file: https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_files/public_list_basic_v2_26may_2017.csv\n       \n### Basic Set versus Primary Set\n\n> The difference between the `basic` and `primary` data sets is that the signals simulated in the `basic` set have, on average, much higher signal to noise ratio (they are larger amplitude signals). They also have other characteristics that will make the different signal classes very distinguishable. **You should be able to get very high signal classification accuracy with the basic data set.**  The primary data set has smaller amplitude signals and can look more similar to each other, making classification accuracy more difficult with this data set. There are also only 4 classes in the basic data set and 7 classes in the primary set. \n\n\n<hr>\n# Primary Training Data Sets\n\n\nDuring the code challenge you have access to a the `full` primary data set and a `small` and `medium` sized subset. \n\n### Primary Small Data Set\n\nThe `primary small` is a subset of the full primary data set.  Use for early-stage prototyping.\n\n  * This data set contains\n    * All 7 signal classifications\n    * 1,000 simulations per class \n    * 7,000 data files (7 classes * 1,000 simulations)\n  * Data ZIP file (2 GB): \n    * File 1/1: https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_v2_zipped/primary_small.zip\n  * Index file: https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_files/public_list_primary_v2_small_1june_2017.csv (&lt;1 MB)\n\n### Primary Medium Data Set\n\nThe `primary medium` is a subset of the full primary data set.  Use for early-stage prototyping & model building.\n\n  * This data set contains\n   * All 7 signal classifications\n   * 10,000 simulations per class \n   * 70,000 data files (7 classes * 10,000 simulations)\n  * Large enough for relatively robust model construction\n  * Data ZIP files (20GB):\n     * File 1/6: https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_v2_zipped/primary_medium_1.zip\n     * File 2/6: https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_v2_zipped/primary_medium_2.zip\n     * File 3/6: https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_v2_zipped/primary_medium_3.zip\n     * File 4/6: https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_v2_zipped/primary_medium_4.zip\n     * File 5/6: https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_v2_zipped/primary_medium_5.zip\n     * File 6/6: https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_v2_zipped/primary_medium_6.zip\n  * Index file: https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_files/public_list_primary_v2_medium_1june_2017.csv (3.5 MB)\n \n### Primary Full Data Set\n\nThe `primary full` is the entire primary data set.  Use only if you want an enourmous training data set. You will need a small data center to process these data in a reasonable amount of time. \n\n  * This data set contains\n    * All 7 signal classifications\n    * 50,000 simulations per class \n    * 350,000 data files files (7 classes * 50,000 simulations)\n  * Data files (130 GB):\n    * 350k individual files\n    * One must read through the index file and download files individually, which will take some time from outside of IBM Cloud systems\n  * Index file: https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_files/public_list_primary_v2_full_1june_2017.csv (17 MB)\n\n<hr>\n# Test Data Set\n\nThere is one `primary_test` data set. Each data file is the same as the above training data except the JSON header does NOT contain the 'signal_classification' key. \n\n  * This data set contains\n    * All 7 signal classifications\n    * ~1,000 simulations per class (+- 50) \n    * 7,014 total files\n  * Data files only include the UUID in the header but not the classification\n  * Data ZIP files (2GB):\n    * File 1/1: https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_v2_zipped/primary_testset.zip\n  * Index file: https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_files/public_list_primary_testset_1k_1june_2017.csv (&lt;1 MB)\n\n> **Submitting Classification Results**\n> See the [Judging Criteria](../Judging_Criteria.ipynb) notebook for information on submitting your test-set classifications."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true
            },
            "source": "<hr>\n<hr>\n<br>\n\n# Programmatically Accessing the Data\n\nThe data are stored in `containers` on IBM Object Storage. You can access these data with HTTP calls. Here we use system level `curl`, but you could easily use the Python `requests` package. \n\nThe URL for all data files is composed of  `base_url/container/objectname`.\n \nThe `base_url` is:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "deletable": true,
                "editable": true
            },
            "outputs": [],
            "source": "#If you are running this in IBM Apache Spark (via Data Science Experience)\nbase_url = 'https://dal05.objectstorage.service.networklayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b'\n\n#ELSE, if you are outside of IBM:\n#base_url = 'https://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b'\n\n#NOTE: if you are outside of IBM, pulling down data will be slower. :/"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "deletable": true,
                "editable": true
            },
            "outputs": [],
            "source": "#Defining a local data folder to dump data\nimport os\n\nmydatafolder = os.path.join( os.environ['PWD'], 'my_data_folder' )\nif os.path.exists(mydatafolder) is False:\n    os.makedirs(mydatafolder)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true
            },
            "source": "## Accessing the Basic Data Set\n\nDownload the basic data set ZIP file and index file."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "deletable": true,
                "editable": true
            },
            "outputs": [],
            "source": "# download the data ZIP file\nbasic_container = 'simsignals_basic_v2'\nbasic4_zip_file = 'basic4.zip'\nos.system('curl {}/{}/{} > {}'.format(base_url, basic_container, basic4_zip_file, mydatafolder + '/' + basic4_zip_file))\n!ls -al $mydatafolder/$basic4_zip_file\n\n# download the index csv file\nbasic4_csv_filename = 'public_list_basic_v2_26may_2017.csv'\nbasic4_csv_url = '{}/simsignals_files/{}'.format(base_url, basic4_csv_filename)\nos.system('curl {} > {}'.format(basic4_csv_url, mydatafolder +'/'+ basic4_csv_filename))\n!ls -al $mydatafolder/$basic4_csv_filename"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true
            },
            "source": "## Accessing the Primary Data Sets\n\n\n### Accessing the Primary Small Data Set\n\nDownload the primary small data set ZIP file and index file."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "deletable": true,
                "editable": true
            },
            "outputs": [],
            "source": "# download the data ZIP file\nprimary_small_filename = 'primary_small.zip'\nprimary_small_url = '{}/simsignals_v2_zipped/{}'.format(base_url, primary_small_filename)\nos.system('curl {} > {}'.format(primary_small_url, mydatafolder +'/'+ primary_small_filename))\n!ls -al $mydatafolder/$primary_small_filename\n\n# download the index csv file\nprimary_small_csv_filename = 'public_list_primary_v2_small_1june_2017.csv'\nprimary_small_csv_url = '{}/simsignals_files/{}'.format(base_url, primary_small_csv_filename)\nos.system('curl {} > {}'.format(primary_small_csv_url, mydatafolder +'/'+ primary_small_csv_filename))\n!ls -al $mydatafolder/$primary_small_csv_filename"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true
            },
            "source": "### Accessing the Primary Medium Data Set\n\nDownload the primary medium data set ZIP files `simignals_v2_zipped/primary_medium_1.zip` ... `simignals_v2_zipped/primary_medium_N.zip` and index file `public_list_primary_v2_medium_1june_2017.csv`"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "deletable": true,
                "editable": true
            },
            "outputs": [],
            "source": "# download the data ZIP file\nmed_N = '{}/simsignals_v2_zipped/primary_medium_{}.zip'\nfor i in range(1,7):\n    med_url = med_N.format(base_url, i)\n    output_file = mydatafolder + '/primary_medium_{}.zip'.format(i)\n    print 'GETing', output_file\n    os.system('curl {} > {}'.format(med_url, output_file ))\n!ls -al $mydatafolder/primary_medium_*.zip\n    \n# download the index csv file    \nprimary_medium_csv_filename = 'public_list_primary_v2_medium_1june_2017.csv'\nmed_csv_url = '{}/simsignals_files/{}'.format(base_url, primary_medium_csv_filename)\nos.system('curl {} > {}'.format(med_csv_url, mydatafolder +'/' + primary_medium_csv_filename))    \n!ls -al $mydatafolder/$primary_medium_csv_filename"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true
            },
            "source": "### Accessing the Primary Full Data Set\n\nDownload the index file for the full data set and the 350k data files, one file at a time. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "deletable": true,
                "editable": true
            },
            "outputs": [],
            "source": "primary_full_csv_filename = 'public_list_primary_v2_full_1june_2017.csv'\nprim_full = '{}/simsignals_files/{}'.format(base_url, primary_full_csv_filename)\nos.system('curl {} > {}'.format(prim_full, mydatafolder +'/' + primary_full_csv_filename))\n!ls -al $mydatafolder/$primary_full_csv_filename"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true
            },
            "source": "> Download this list and begin to pull down files individually if desired. Warning, **however, this will take approximately a billion years if you are not running on IBM Apache Spark** -- IBM Apache Spark and Object Storage exist in the same data center and share a fast network connection. \n\nThe data are found in `base_url/simsignals_v2/<uuid>.dat`\n\nExample data file URL:\n\nhttps://dal.objectstorage.open.softlayer.com/v1/AUTH_cdbef52bdf7a449c96936e1071f0a46b/simsignals_v2/aa7d082f-9263-4533-a9d4-5595c5cdde25.dat\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "deletable": true,
                "editable": true
            },
            "outputs": [],
            "source": "import requests\nimport copy\nfile_list_container = 'simsignals_files'\nfile_list = 'public_list_primary_v2_full_1june_2017.csv'\nprimary_data_container = 'simsignals_v2'\nr = requests.get('{}/{}/{}'.format(base_url, file_list_container, file_list), timeout=(9.0, 21.0))\nfilecontents = copy.copy(r.content)\nfull_primary_files = [line.split(',') for line in filecontents.split('\\n')]\nfull_primary_files = full_primary_files[1:-1] #strip the header and empty last element\nfull_primary_files = map(lambda x: x[0]+\".dat\", full_primary_files)  #now list of file names (<uuid>.dat)\n\n#save your data into a local subfolder\nsave_to_folder = mydatafolder + '/primary_data_set'\nif os.path.exists(save_to_folder) is False:\n    os.mkdir(save_to_folder)\n\ncount = 0\ntotal = len(full_primary_files)\nfor row in full_primary_files:\n    r = requests.get('{}/{}/{}'.format(base_url, primary_data_container, row), timeout=(9.0, 21.0))\n    \n    if count % 100 == 0:\n        print 'done ', count, ' out of ',  total\n    count += 1\n    \n    with open('{}/{}'.format(save_to_folder, row), 'w' ) as fout:\n        fout.write(r.content)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true
            },
            "source": "> This will be a difficult data set to consume and process if you are using free-tier levels of software from any Cloud provider. You will likely want to have a robust machine, or sets of machines, with many threads and GPUs if you want to train models with such a large dat set. \n\n> For example, if you have access to an IBM Spark Enterprise cluster, because the network connection between IBM Spark and IBM Object Storage is so fast, we recommend that you **do NOT** download each file. Instead you could parallelize the index file and then retrieve and process each file on a worker node. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "deletable": true,
                "editable": true
            },
            "outputs": [],
            "source": "## Using Spark -- can parallelize the job across your worker nodes\nimport ibmseti\ndef retrieve_and_process(row):\n    try:\n        r = requests.get('{}/{}/{}'.format(base_url, primary_data_container, row), timeout=(9.0, 21.0))\n    except Exception as e:\n        return (row, 'failed', [])\n    \n    aca = ibmseti.compamp.SimCompamp(r.content)\n    spectrogram = aca.get_spectrogram() # or do something else\n    features = my_feature_extractor(spectrogram) #example external function for reducing the spectrogram into a handful of features, perhaps\n    \n    signal_class = aca.header()['signal_classifiation']\n        \n    return (row, signal_class, features)\n\nnpartitions = 60  \nrdd = sc.parallelize(full_primary_files, npartitions)\n\n#Now ask Spark to run the job\nprocess_results = rdd.map(retrieve_and_process).collect()"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true
            },
            "source": "## Accessing the Test Data Set\n\nOnce you've trained your model, done all of your testing, and tweaks and are ready to submit an entry to the contest, you'll need to download the test data set and apply your model to that.  \n\nThe test data set is similar to the labeled data, except that the JSON header is missing the 'signal_classification' key, and just contains the 'uuid'. \n\nLike the other sets, this set is found in a `.zip` file in the `simsignals_v2_zipped` container;"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "deletable": true,
                "editable": true
            },
            "outputs": [],
            "source": "# download the test data ZIP file\ntestset_filename = 'primary_testset.zip'\ntest_set_url = '{}/simsignals_v2_zipped/{}'.format(base_url, testset_filename)\nos.system('curl {} > {}'.format(test_set_url, mydatafolder +'/'+testset_filename))\n!ls -al $mydatafolder/$testset_filename\n\n# download the test index csv file\ntestset_csv_filename = 'public_list_primary_testset_1k_1june_2017.csv'\ntest_set_csv_url = '{}/simsignals_files/{}'.format(base_url, testset_csv_filename)\nos.system('curl {} > {}'.format(test_set_csv_url, mydatafolder + '/' + testset_csv_filename))\n!ls -al $mydatafolder/$testset_csv_filename"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 1.6",
            "language": "python",
            "name": "python2"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 2
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython2",
            "version": "2.7.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
